{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa26c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_13200\\355526377.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=local_llm, base_url=url_llm, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "local_llm = \"llama3.2:latest\"\n",
    "url_llm = \"http://localhost:11434\"\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=local_llm, base_url=url_llm, temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab47e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks and an expert in research projects funded by the European Union under the Horizon 2020 programme.\n",
    "Use the following context from Horizon 2020 projects to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19ff2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"What is the objective of the project with grant agreement 740934?\"\n",
    "query_2 = \"What is the total cost of the project with the acronym HYPERGRYD?\"\n",
    "query_3 = \"How much funding was allocated for the project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration?\"\n",
    "query_4 = \"Which organisation played the role of coordinator in the grant agreement 777998?\"\n",
    "query_5 = \"What topic does the project with the acronym INTERRFACE belong to?\"\n",
    "query_6 = \"What legal basis was the project titled European Joint Programme on Radioactive Waste Management framed within?\"\n",
    "query_7 = \"What type of proposal was the grant agreement 814416?\"\n",
    "query_8 = \"To which master call was the project with the acronym G9NIGHT submitted?\"\n",
    "query_9 = \"To which sub call was the project titled Electron Nanocrystallography submitted?\"\n",
    "query_10 = \"Provide the grant agreement of 1 project which objective is related to artificial intelligence.\"\n",
    "query_11 = \"Provide the acronym of 1 project which objective is related to robotics.\"\n",
    "query_12 = \"Provide the title of 1 project which objective is related to geolocation.\"\n",
    "query_13 = \"Provide the objective of 1 project related to digital twin.\"\n",
    "query_14 = \"Provide the objective of 3 different projects related to corrosion.\"\n",
    "query_15 = \"Provide the title of 3 different projects which objective is related to offshore structures.\"\n",
    "query_16 = \"Provide the acronym of 3 different projects which objective is related to materials engineering.\"\n",
    "query_17 = \"Provide the grant agreement of 3 different projects which objective is related to nanocomposites.\"\n",
    "query_18 = \"Provide the name of an organisation that has participated in projects which objective is related to artificial intelligence.\"\n",
    "query_19 = \"Provide the name of an organisation which activity type is PRC and that has participated in projects which objective is related to robotics.\"\n",
    "query_20 = \"Provide the PIC of an organisation that is a small or medium enterprise and has participated in projects which objective is related to geolocation.\"\n",
    "query_21 = \"Provide the name of an organisation that has played the role of coordinator in projects which objective is related to digital twin.\"\n",
    "query_22 = \"Provide the PIC of a Spanish organisation that has participated in projects which objective is related to corrosion.\"\n",
    "query_23 = \"Provide the name of an european organisation that has participated in projects which objective is related to offshore structures.\"\n",
    "query_24 = \"Provide the PIC of an european small or medium enterprise that has participated in projects which objective is related to materials engineering.\"\n",
    "query_25 = \"Provide the name of an european small or medium enterprise that has played the role of coordinator in projects which objective is related to nanocomposites.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e8e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_13200\\2374641863.py:7: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # Al no tener NVIDIA es necesario cambiarlo model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c4aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore_7_txt = FAISS.load_local(\"faiss_index_proyectos_7_txt\", embeddings=emb, \n",
    "                                 allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e95a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List\n",
    "\n",
    "retriever_multi_7_txt = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore_7_txt.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=prompt_5best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6baf8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_13200\\2218446891.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  filter_chain = LLMChain(llm=llm, prompt=filter_prompt)\n"
     ]
    }
   ],
   "source": [
    "prompt_5best = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert at rewriting queries. Given the original query, \n",
    "generate 10 variations that would return the most relevant documents.\n",
    "\n",
    "Original: {question}\n",
    "\n",
    "Your 5 best query variations:\n",
    "1.\"\"\")\n",
    "\n",
    "filter_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Contenido del documento:\n",
    "{document}\n",
    "\n",
    "¿Este documento responde (al menos parcialmente) a la pregunta?\n",
    "Responde solo \"Sí\" o \"No\".\n",
    "\"\"\")\n",
    "filter_chain = LLMChain(llm=llm, prompt=filter_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb01c4",
   "metadata": {},
   "source": [
    "## RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6c73438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_docs(docs: List):\n",
    "    docs_filtrados = []\n",
    "    for doc in docs:\n",
    "        respuesta = filter_chain.run({\"question\": current_query, \"document\": doc.page_content})\n",
    "        if respuesta.strip().lower().startswith((\"sí\", \"yes\", \"Sí\", \"Yes\")):\n",
    "            docs_filtrados.append(doc)\n",
    "    return docs_filtrados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3249733",
   "metadata": {},
   "source": [
    "RunnableLambda se usa con el fin de que una función de Python pueda encajar dentro de un pipeline de LangChain que espera.\n",
    "\n",
    "RunnablePassthrough se usa para pasar el valor de entrada al realizar el invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39b66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_docs = RunnableLambda(lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
    "\n",
    "rag_chain_multi_7_txt = (\n",
    "    {\n",
    "        \"context\": retriever_multi_7_txt \n",
    "                    | filtrado_llm \n",
    "                    | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "090757bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Tiempo = 738.3345 s \n",
      "\n",
      "La respuesta es: I don't know the answer to your question about the grant agreement 740934. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resultados = {}\n",
    "\n",
    "for i in range(1,2):  \n",
    "    inicio = time.time()\n",
    "    \n",
    "    current_query = eval(f\"query_{i}\")\n",
    "    answer = rag_chain_multi_7_txt.invoke(current_query)\n",
    "    \n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "    \n",
    "    resultados[f\"query_{i}\"] = answer\n",
    "    \n",
    "    print(f\"Query {i}: Tiempo = {tiempo_ejecucion:.4f} s \\n\")\n",
    "    print(f\"La respuesta es: {answer} \\n\")\n",
    "    print(\"\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cabb57",
   "metadata": {},
   "source": [
    "## Filtro de manera más manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66167da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m filtered_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocs\u001b[49m:\n\u001b[0;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m filter_chain\u001b[38;5;241m.\u001b[39mrun({\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query_1,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[0;32m      7\u001b[0m     })\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    response = filter_chain.run({\n",
    "        \"question\": query_1,\n",
    "        \"document\": doc.page_content\n",
    "    })\n",
    "    print(response)\n",
    "    if response.strip().lower().startswith(\"sí\") or response.strip().lower().startswith(\"Sí\") or response.strip().lower().startswith(\"Yes\"):  \n",
    "        filtered_docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1d3c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_multi_7_txt = (\n",
    "    {\"context\": retriever_multi_7_txt | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c8da73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the specific grant agreement number you are referring to. However, I can tell you that CLASP (Cultural Landscape of Anglo-Saxon Poetry) has a grant agreement number 740934 and its objective is to create an online and interactive consolidated library of Anglo-Saxon poetry. The project aims to produce a comprehensive digital tool for the study of Anglo-Saxon verse, marking up over 60,000 lines of poetry in TEI P5 XML.\n"
     ]
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38f65",
   "metadata": {},
   "source": [
    "## FILTRO MANUAL EN EL CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff30b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27552f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Tiempo = 658.6796 s\n",
      "La respuesta es: I don't know the objective of the project with grant agreement 740934.\n",
      "\n",
      "The objective of the SPOTVIEW project is to develop and demonstrate innovative, sustainable and efficient processes and technology components, in order to optimize the use of natural resources, especially water, in three industrial sectors (Dairy, Pulp and Paper and Steel).\n",
      "\n",
      "I don't know the objective of the MiLC project.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 2: Tiempo = 635.0576 s\n",
      "La respuesta es: I don't know the specific details about the HYPERGRYD project, including its total cost. However, I can suggest checking the official Horizon 2020 website or contacting the European Commission for more information on this project. If available, the budget and costs of the project might be listed in the project's deliverables or reports.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 3: Tiempo = 783.2616 s\n",
      "La respuesta es: I couldn't find any information on a Horizon 2020 project with the title \"Transforming Research through Innovative Practices for Linked interdisciplinary Exploration\". I don't know the answer to this question. If you provide more context or details, I may be able to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 4: Tiempo = 428.2154 s\n",
      "La respuesta es: I don't have access to specific information about the grant agreement 777998, including which organization played the role of coordinator. Horizon 2020 projects are vast and diverse, and without more context or details, it's challenging for me to provide an accurate answer. If you could provide more information or clarify which project this is related to, I'd be happy to try and assist further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 5: Tiempo = 746.8284 s\n",
      "La respuesta es: I don't know which Horizon 2020 project INTERRFACE belongs to.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 6: Tiempo = 1088.8689 s\n",
      "La respuesta es: The European Joint Programme on Radioactive Waste Management (EURAD) was framed within the Horizon 2020 programme. Specifically, it is a Coordination and Support Action (CSA). This type of action allows for coordination and support of research activities across EU Member States.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 7: Tiempo = 1014.0492 s\n",
      "La respuesta es: I don't have information about a specific grant agreement with number 814416. Horizon 2020 proposals are typically categorized as either Research & Innovation (R&I) or Innovation (InnoVA), but without more context, I couldn't determine the exact type of proposal associated with this grant agreement. If you provide more details or clarify which project it belongs to, I'll try to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 8: Tiempo = 683.0484 s\n",
      "La respuesta es: I don't know. I couldn't find any information on a Horizon 2020 project with the acronym G9NIGHT. If you provide more context or details, I may be able to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 9: Tiempo = 503.9234 s\n",
      "La respuesta es: I don't know the specific sub-call to which the project \"Electron Nanocrystallography\" was submitted. However, I can suggest checking the official Horizon 2020 project database or contacting the European Commission for more information. The project might have been submitted under a different call or program.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 10: Tiempo = 792.3262 s\n",
      "La respuesta es: I can provide information on Horizon 2020 projects related to artificial intelligence. One example is the \"AI for Europe\" project, which aimed to develop a comprehensive strategy for the European Union's approach to Artificial Intelligence (AI). The grant agreement for this project was signed in 2018 and ran until 2021.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 11: Tiempo = 450.2640 s\n",
      "La respuesta es: The acronym for a Horizon 2020 project related to robotics is SMOOTH, which stands for Smart Multi-Operational Humanoid Robot for Firefighting Operations and Rescue Missions.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 12: Tiempo = 615.8884 s\n",
      "La respuesta es: I'm not aware of a specific Horizon 2020 project with an objective directly related to geolocation. However, I can suggest that the \"Geo-Informatics for Environmental Management\" (GEM) project might be relevant as it involves the use of geographic information systems and spatial analysis. If you need more information, please provide more context or details about the specific objective you're looking for.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 13: Tiempo = 437.4871 s\n",
      "La respuesta es: The objective of the B¦IMprove project is to facilitate a comprehensive end-to-end digital thread in construction by using autonomous tracking systems to continuously identify deviations and update Digital Twins accordingly. This aims to optimize resource allocation, personnel flow, and safety on construction sites. The system provides personalized interfaces for easy access to information for all user groups.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 14: Tiempo = 696.6628 s\n",
      "La respuesta es: I can provide information on Horizon 2020 projects related to corrosion. Here are three examples:\n",
      "\n",
      "1. CorroCor (2015-2018): The objective of this project was to develop a new approach for the prediction and prevention of corrosion in industrial applications, focusing on the use of machine learning algorithms.\n",
      "2. Corrosion in Advanced Materials (CORAMAT) (2017-2020): This project aimed to investigate the corrosion behavior of advanced materials used in various industries, such as aerospace and automotive, with a focus on developing new testing methods and standards.\n",
      "3. SmartCorr (2018-2021): The objective of this project was to develop smart coatings for corrosion protection in industrial applications, focusing on the use of nanomaterials and advanced manufacturing techniques.\n",
      "\n",
      "Please note that these objectives might not be exhaustive or up-to-date, as my knowledge cutoff is December 2023.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 15: Tiempo = 424.0616 s\n",
      "La respuesta es: Based on my knowledge, here are three Horizon 2020 project titles with an objective related to offshore structures:\n",
      "\n",
      "1. \"OceanGate\" - a project focused on developing and demonstrating advanced technologies for the construction of offshore wind farms.\n",
      "2. \"DeepSeaNet\" - a project aiming to develop new materials and techniques for the fabrication of deepwater offshore structures.\n",
      "3. \"COSMOS\" - a project exploring the use of artificial intelligence and machine learning in the design, construction, and operation of offshore structures.\n",
      "\n",
      "Please note that these titles are subject to change, and I may not have access to the most up-to-date information.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 16: Tiempo = 627.0091 s\n",
      "La respuesta es: Based on my knowledge of Horizon 2020 projects, here are three acronyms related to materials engineering:\n",
      "\n",
      "1. MATEM (Materials for Advanced Technologies and Energy Management)\n",
      "2. FAMICOM (Functional Materials for Innovative Composites and Operations Management)\n",
      "3. NANOCOAT (Nanocoatings for Corrosion Protection)\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 17: Tiempo = 654.4572 s\n",
      "La respuesta es: I can provide some information on Horizon 2020 projects related to nanocomposites, but please note that I may not have access to specific grant agreements.\n",
      "\n",
      "Here are three examples of Horizon 2020 projects with objectives related to nanocomposites:\n",
      "\n",
      "1. Project: \"NanoComposites for Energy Applications\" (GA No: 764372) - This project aims to develop new nanocomposite materials for energy storage and conversion applications.\n",
      "2. Project: \"Nanoparticles and Nanocomposites for Medical Applications\" (GA No: 690657) - This project focuses on the development of nanoparticles and nanocomposites for medical imaging, diagnostics, and therapy.\n",
      "3. Project: \"NanoComposites for Advanced Materials and Technologies\" (GA No: 777853) - This project explores the development of new nanocomposite materials with improved mechanical, thermal, and electrical properties.\n",
      "\n",
      "Please note that I don't have direct access to the grant agreements, but these projects are publicly available information.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 18: Tiempo = 579.5813 s\n",
      "La respuesta es: The European Cybercrime Centre (EC3) is an organisation that has participated in Horizon 2020 projects related to artificial intelligence in law enforcement. EC3 is a joint initiative between Europol and Interpol, aiming to enhance cooperation and information sharing among law enforcement agencies across Europe. As part of the Horizon 2020 programme, EC3 has been involved in various projects focused on AI-powered solutions for law enforcement.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 19: Tiempo = 744.7253 s\n",
      "La respuesta es: I'm not aware of a specific organisation with the exact combination of characteristics you've described, including being under Horizon 2020 and having a project related to robotics. However, I can suggest some EU-funded organisations that have participated in robotics-related projects under Horizon 2020, such as the European Robotics Association (ERA) or the Robotics for All initiative. If you provide more information, I may be able to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 20: Tiempo = 579.8024 s\n",
      "La respuesta es: I don't have enough information to provide a specific PIC (Personal Identification Code) for an organization. However, I can suggest some EU-funded Horizon 2020 projects related to geolocation, such as the \"GeoSolutions\" project or the \"Geospatial Intelligence\" project. These projects involved SMEs in various European countries and might have provided information on their participation.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 21: Tiempo = 893.5346 s\n",
      "La respuesta es: I don't know the name of an organisation that has played the role of coordinator in projects related to digital twin, specifically under Horizon 2020. However, I can suggest some EU-funded projects that focus on digital twins, such as IOTWINS or Digital Twin Consortium. If you need more information, please let me know and I'll try to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 22: Tiempo = 617.4616 s\n",
      "La respuesta es: I don't have information on specific projects or organisations participating in Horizon 2020 projects related to corrosion. However, I can suggest some possible sources where you may be able to find the required information, such as the EU's Horizon 2020 website or research databases like ResearchGate or Scopus. If you provide more context or details, I may be able to help further.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 23: Tiempo = 640.7769 s\n",
      "La respuesta es: One European organization that comes to mind is DNV GL (Det Norske Veritas), a Norwegian-based company that has participated in several Horizon 2020 projects related to offshore structures, such as the \"Offshore Renewable Energy\" project. They have expertise in designing and constructing offshore wind farms and other renewable energy infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 24: Tiempo = 581.7106 s\n",
      "La respuesta es: I don't have access to real-time information about specific European SMEs participating in Horizon 2020 projects. However, I can suggest some ways for you to find the PIC (Personal Identification Code) of a European SME that has participated in a project related to materials engineering. You may want to search through the EU's Horizon 2020 database or contact the relevant project coordinator directly.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Query 25: Tiempo = 578.4722 s\n",
      "La respuesta es: I'm not aware of a specific European SME that has played the role of coordinator in Horizon 2020 projects focused on nanocomposites. However, I can suggest some EU-funded projects that involve nanocomposites, such as \"Nanomaterials for Energy Applications\" (NANOMAT) or \"Graphene and 2D Materials for Energy Storage\" (GRAPHENE-ES). If you need more information, please let me know.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "resultados={}\n",
    "for i in range(1, 26):\n",
    "    inicio = time.time()\n",
    "    current_query = eval(f\"query_{i}\")\n",
    "\n",
    "    # Recuperar documentos\n",
    "    #raw_docs = retriever_multi_7_txt.invoke(current_query)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        filtered_docs = []\n",
    "        for doc in docs #raw_docs:\n",
    "            response = filter_chain.run({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            if response.strip().lower().startswith((\"sí\", \"yes\", \"Sí\")):\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "\n",
    "    rag_chain_multi_7_txt = (\n",
    "        {\"context\": retriever_multi_7_txt | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    # Ejecutar RAG con los docs filtrados\n",
    "    answer = rag_chain_multi_7_txt.invoke(current_query)\n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "\n",
    "    resultados[f\"query_{i}\"] = answer\n",
    "\n",
    "    print(f\"Query {i}: {current_query} \\n Tiempo = {tiempo_ejecucion:.4f} s\")\n",
    "    print(f\"La respuesta es: {answer}\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4649ef2",
   "metadata": {},
   "source": [
    "## NOTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2121014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_19752\\3047099389.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  filter_chain = LLMChain(llm=llm, prompt=filter_prompt)\n"
     ]
    }
   ],
   "source": [
    "filter_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Contenido del documento:\n",
    "{document}\n",
    "\n",
    "Del 0 al 10, ¿qué tan bien responde este documento a la pregunta?\n",
    "Responde solo con un número entero del 0 al 10.\n",
    "\"\"\")\n",
    "filter_chain = LLMChain(llm=llm, prompt=filter_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c2338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2487ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_19752\\3807181424.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = filter_chain.run({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Query 1: Tiempo = 596.8999 s\n",
      "La respuesta es: I don't have information on a specific Horizon 2020 project with grant agreement 740934. I can try to search for more context or details about the project, but without further information, I couldn't find any relevant data. If you provide more context or details about the project, I'll do my best to help.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 0\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Filtro LLM: 7\n",
      "Query 2: Tiempo = 669.4420 s\n",
      "La respuesta es: I don't know the specific details about the HYPERGRYD project, including its total cost. However, I can suggest checking the official Horizon 2020 website or contacting the European Commission for more information on this project. If available, the budget and costs of the project might be listed in the project's deliverables or reports.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_chain = LLMChain(llm=llm, prompt=filter_prompt)\n",
    "import time\n",
    "resultados={}\n",
    "for i in range(1, 3):\n",
    "    inicio = time.time()\n",
    "    current_query = eval(f\"query_{i}\")\n",
    "\n",
    "    # Recuperar documentos\n",
    "    raw_docs = retriever_multi_7_txt.invoke(current_query)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        filtered_docs = []\n",
    "        for doc in docs:\n",
    "            response = filter_chain.run({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            print(f\"Filtro LLM: {response}\")\n",
    "            try:\n",
    "                score = int(response.strip())\n",
    "                if score > 7:\n",
    "                    filtered_docs.append(doc)\n",
    "            except ValueError:\n",
    "                response = filter_chain.run({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "                })\n",
    "                print(f\"Filtro LLM: {response}\")\n",
    "                try:\n",
    "                    score = int(response.strip())\n",
    "                    if score > 7:\n",
    "                        filtered_docs.append(doc)\n",
    "\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "\n",
    "    rag_chain_multi_7_txt = (\n",
    "        {\"context\": retriever_multi_7_txt | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    # Ejecutar RAG con los docs filtrados\n",
    "    answer = rag_chain_multi_7_txt.invoke(current_query)\n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "\n",
    "    resultados[f\"query_{i}\"] = answer\n",
    "\n",
    "    print(f\"Query {i}: Tiempo = {tiempo_ejecucion:.4f} s\")\n",
    "    print(f\"La respuesta es: {answer}\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d35a9",
   "metadata": {},
   "source": [
    "## EXP 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aef528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore_8_txt = FAISS.load_local(\"faiss_index_proyectos_8_txt\", embeddings=emb, \n",
    "                                 allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi_8_txt = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore_8_txt.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=prompt_5best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f02f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resultados={}\n",
    "for i in range(1, 26):\n",
    "    inicio = time.time()\n",
    "    current_query = eval(f\"query_{i}\")\n",
    "\n",
    "    # Recuperar documentos\n",
    "    raw_docs = retriever_multi_8_txt.invoke(current_query)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        filtered_docs = []\n",
    "        for doc in raw_docs:\n",
    "            response = filter_chain.run({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            if response.strip().lower().startswith((\"sí\", \"yes\", \"Sí\")):\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "\n",
    "    rag_chain_multi_8_txt = (\n",
    "        {\"context\": retriever_multi_8_txt | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    # Ejecutar RAG con los docs filtrados\n",
    "    answer = rag_chain_multi_8_txt.invoke(current_query)\n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "\n",
    "    resultados[f\"query_{i}\"] = answer\n",
    "\n",
    "    print(f\"Query {i}: Tiempo = {tiempo_ejecucion:.4f} s\")\n",
    "    print(f\"La respuesta es: {answer}\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3bfee",
   "metadata": {},
   "source": [
    "## EXP 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbeb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name_9 = \"intfloat/e5-small-v2\"\n",
    "model_kwargs_9 = {\"device\": \"cpu\"}\n",
    "encode_kwargs_9={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    "\n",
    "emb_9 = HuggingFaceBgeEmbeddings(model_name=model_name_9,model_kwargs=model_kwargs_9,encode_kwargs=encode_kwargs_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d403bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore_9_txt = FAISS.load_local(\"faiss_index_proyectos_9_txt\", embeddings=emb_9, \n",
    "                                 allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi_9_txt = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore_9_txt.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=prompt_5best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resultados={}\n",
    "for i in range(1, 26):\n",
    "    inicio = time.time()\n",
    "    current_query = eval(f\"query_{i}\")\n",
    "\n",
    "    # Recuperar documentos\n",
    "    raw_docs = retriever_multi_8_txt.invoke(current_query)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        filtered_docs = []\n",
    "        for doc in raw_docs:\n",
    "            response = filter_chain.run({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            if response.strip().lower().startswith((\"sí\", \"yes\", \"Sí\")):\n",
    "                filtered_docs.append(doc)\n",
    "\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "\n",
    "    rag_chain_multi_8_txt = (\n",
    "        {\"context\": retriever_multi_8_txt | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    # Ejecutar RAG con los docs filtrados\n",
    "    answer = rag_chain_multi_8_txt.invoke(current_query)\n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "\n",
    "    resultados[f\"query_{i}\"] = answer\n",
    "\n",
    "    print(f\"Query {i}: Tiempo = {tiempo_ejecucion:.4f} s\")\n",
    "    print(f\"La respuesta es: {answer}\\n\\n---\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 REAL",
   "language": "python",
   "name": "py310real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
