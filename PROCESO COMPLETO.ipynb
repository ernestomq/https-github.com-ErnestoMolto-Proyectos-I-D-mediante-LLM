{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9622c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema import StrOutputParser, Document\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Optional\n",
    "from functools import lru_cache\n",
    "from langchain.storage import InMemoryStore\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba711a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi= \"phi3:mini\"\n",
    "llama = \"llama3.2:latest\"\n",
    "url_llm = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d47d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_llm = OllamaLLM(model=phi, base_url=url_llm, temperature=0)\n",
    "llama_llm = OllamaLLM(model=llama, base_url=url_llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ad4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks and an expert in research projects funded by the European Union under the Horizon 2020 programme.\n",
    "Use the following context from Horizon 2020 projects to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_evaluation = \"\"\"\n",
    "    You are a rigorous evaluator. You will compare a system-generated answer to the known correct answer and evaluate its accuracy.\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Analyze the question, the correct answer, and the generated answer.\n",
    "    2. Evaluate EXCLUSIVELY the match between the generated answer and the correct answer.\n",
    "    3. Assigns a score from 1 to 5 based on this scale:\n",
    "\n",
    "    1 = Completely incorrect (no match in any respect)\n",
    "    2 = Mostly incorrect (minimal match)\n",
    "    3 = Partially correct (some elements match)\n",
    "    4 = Mostly correct (only minor errors)\n",
    "    5 = Completely correct (exact match or equivalent).\n",
    "\n",
    "    DATA:\n",
    "    Question: {question}\n",
    "    Correct answer: {answer_correct}\n",
    "    Response generated: {answer}\n",
    "\n",
    "    STRICT OUTPUT FORMAT (no deviations):\n",
    "    Justification: [1-2 sentence concise explanation of differences/similarities]\n",
    "    Score: [Integer from 1 to 5]\n",
    "    \"\"\"\n",
    "prompt_evaluation_with_context =\"\"\"\n",
    "You are an advanced evaluator. You must evaluate:\n",
    "1. The accuracy of the generated answer compared to the correct answer.\n",
    "2. The relevance of the context used.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Analyse the question, the correct answer, the generated answer and the context.\n",
    "- Evaluate whether the context adequately supports the answer.\n",
    "- Assign two scores (1-10):\n",
    "\n",
    "- Quality of the answer:\n",
    "1 = Totally incorrect\n",
    "10 = Totally correct.\n",
    "\n",
    "- Quality of the context:\n",
    "1 = Not at all relevant\n",
    "10 = Totally relevant\n",
    "\n",
    "DATA:\n",
    "Question: {question}\n",
    "Correct answer: {answer_correct}\n",
    "Response generated: {answer}\n",
    "Context used: {context}\n",
    "\n",
    "STRICT OUTPUT FORMAT:\n",
    "Response justification: [Concise explanation]\n",
    "Score response: [1-10]\n",
    "Score context: [1-10]\n",
    "\n",
    "The output has to be ONLY THE STRICT OUTPUT FORMAT, with the Score of the answer and the context.\n",
    "\"\"\"\n",
    "\n",
    "filter_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an assistant that determines whether a document provides relevant information to answer a question.\n",
    "\n",
    "If the document contains any information that helps partially or fully answer the question, answer \"Yes\". \n",
    "If it is irrelevant, answer \"No\".\n",
    "\n",
    "ONLY answer with \"Yes\" or \"No\" — no other words or punctuation.\n",
    "\n",
    "Question: {question}\n",
    "Document: {document}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt_evaluation = ChatPromptTemplate.from_template(prompt_evaluation)\n",
    "prompt_evaluation_with_context = ChatPromptTemplate.from_template(prompt_evaluation_with_context)\n",
    "filter_chain = LLMChain(llm=llama_llm, prompt=filter_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03a2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"What is the objective of the project with grant agreement 740934?\"\n",
    "query_2 = \"What is the total cost of the project with the acronym HYPERGRYD?\"\n",
    "query_3 = \"How much funding was allocated for the project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration?\"\n",
    "query_4 = \"Which organisation played the role of coordinator in the grant agreement 777998?\"\n",
    "query_5 = \"What topic does the project with the acronym INTERRFACE belong to?\"\n",
    "query_6 = \"What legal basis was the project titled European Joint Programme on Radioactive Waste Management framed within?\"\n",
    "query_7 = \"What type of proposal was the grant agreement 814416?\"\n",
    "query_8 = \"To which master call was the project with the acronym G9NIGHT submitted?\"\n",
    "query_9 = \"To which sub call was the project titled Electron Nanocrystallography submitted?\"\n",
    "query_10 = \"Provide the grant agreement of 1 project which objective is related to artificial intelligence.\"\n",
    "query_11 = \"Provide the acronym of 1 project which objective is related to robotics.\"\n",
    "query_12 = \"Provide the title of 1 project which objective is related to geolocation.\"\n",
    "query_13 = \"Provide the objective of 1 project related to digital twin.\"\n",
    "query_14 = \"Provide the objective of 3 different projects related to corrosion.\"\n",
    "query_15 = \"Provide the title of 3 different projects which objective is related to offshore structures.\"\n",
    "query_16 = \"Provide the acronym of 3 different projects which objective is related to materials engineering.\"\n",
    "query_17 = \"Provide the grant agreement of 3 different projects which objective is related to nanocomposites.\"\n",
    "query_18 = \"Provide the name of an organisation that has participated in projects which objective is related to artificial intelligence.\"\n",
    "query_19 = \"Provide the name of an organisation which activity type is PRC and that has participated in projects which objective is related to robotics.\"\n",
    "query_20 = \"Provide the PIC of an organisation that is a small or medium enterprise and has participated in projects which objective is related to geolocation.\"\n",
    "query_21 = \"Provide the name of an organisation that has played the role of coordinator in projects which objective is related to digital twin.\"\n",
    "query_22 = \"Provide the PIC of a Spanish organisation that has participated in projects which objective is related to corrosion.\"\n",
    "query_23 = \"Provide the name of an european organisation that has participated in projects which objective is related to offshore structures.\"\n",
    "query_24 = \"Provide the PIC of an european small or medium enterprise that has participated in projects which objective is related to materials engineering.\"\n",
    "query_25 = \"Provide the name of an european small or medium enterprise that has played the role of coordinator in projects which objective is related to nanocomposites.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383d694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_1_correct=\"The project’s objective is to combat violent extremism by analyzing its root causes, developing preventive and repressive measures, and countering extremist narratives through collaboration with civil society and LEAs, all while upholding fundamental rights.\"\n",
    "answer_2_correct =\"The total cost of the project with the acronym HYPERGRYD (grant agreement 101036656) was €5,987,875.00.\"\n",
    "answer_3_correct =\"Transforming Research through Innovative Practices for Linked interdisciplinary Exploration” (TRIPLE), identified by grant agreement 863420, received a total EU contribution of € 5,626,548.75. This funding was allocated as part of Horizon 2020 under the “EXCELLENT SCIENCE – Research Infrastructures\"\n",
    "answer_4_correct =\"\"\"the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998.\n",
    "The organisation with Participant Identification Code (PIC) 960782479 participated in the grant agreement 777998. The name of this organisation is UNIVERSIDADE NOVA DE LISBOA. The organisation with PIC 960782479 is not a small or medium-sized enterprise. The organisation with PIC 960782479 develops an activity of type HES. The organisation with PIC 960782479 is based in the country PT, codified under ISO 3166. The organisation with PIC 960782479 played the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998 was 409500.0 euros. The total amount funded to the organization with PIC 960782479 in the grant agreement 777998 was 409500.0 euros.\n",
    "\"\"\"\n",
    "answer_5_correct =\"The grant agreement 824330 was framed within the topic LC-SC3-ES-5-2018-2020TSO – DSO – Consumer: Large-scale demonstrations of innovative grid services through demand response, storage and small-scale (RES) generation. The grant agreement 824330 was framed within the master call H2020-LC-SC3-2018-2019-2020. The grant agreement 824330 was framed within the subcall H2020-LC-SC3-2018-ES-SCC.\"\n",
    "answer_6_correct =\"The grant agreement 847593 was framed within the legal basis H2020-EuratomEuratom.\"\n",
    "answer_7_correct =\"The grant agreement 814416 was a Research and Innovation Action (RIA) proposal.\"\n",
    "answer_8_correct =\"The grant agreement 101036041 was framed within the master call H2020-MSCA-NIGHT-2020bis.\"\n",
    "answer_9_correct =\"The grant agreement 956099 was framed within the legal basis H2020-EU.1.3.EXCELLENT SCIENCE - Marie Skłodowska-Curie Actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfc33b",
   "metadata": {},
   "source": [
    "## MODELO sentence-transformers/paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcac451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_18236\\4035595027.py:4: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db8782",
   "metadata": {},
   "source": [
    "## EXPERIMENTO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcf7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_7_txt = FAISS.load_local(\"faiss_index_proyectos_7_txt\", embeddings=emb, \n",
    "                                 allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3761706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multi_7_txt = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore_7_txt.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    llm=phi_llm,\n",
    "    include_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c385e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "Query 1: Tiempo = 337.3938 s\n",
      "La respuesta es: The objective of grant agreement 740934 under Horizon 2020 was to provide a conceptual basis for smart provision of public goods by EU agriculture and forestry ecosystems in the context of intensification scenarios. It aimed at offering tools, evidence, policy options, and improved incentives through transdisciplinary approaches while considering legislation impact across multiple scales within thirteen European countries.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "Query 2: Tiempo = 286.7720 s\n",
      "La respuesta es: The total funded cost for each project is explicitly mentioned next to its description, which allows us to directly answer your question without additional research or inference. The grant agreement number and the corresponding costs are as follows: HYPERGRYD (101036656) had a total funded cost of 5,987,874.5 euros; MSCA-IF project with gran agreement number 747921 also had a total funded cost of 183,454.8 euros (noting that this is the same amount as HYPERGRYD's); and finally, for grant agreement number 798271 under MSCA-IF with subcall H2020-MSCA-IF-2017, it had a total funded cost of 183,454.8 euros as well.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "Query 3: Tiempo = 209.5691 s\n",
      "La respuesta es: The project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration had a total funded cost of 137591.1 euros under grant agreement number 707706.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "Query 4: Tiempo = 190.6349 s\n",
      "La respuesta es: The European Commission (EC) acted as the project leader for Grant Agreement No. 777998 under Horizon 2020, while a consortium of partners from various countries took on coordinator roles to manage and execute the research activities effectively across borders.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "Query 5: Tiempo = 226.2266 s\n",
      "La respuesta es: INTERRFACE is related to fostering dialogue with stakeholders for better forest protection within legal frameworks, as part of Horizon 2020 project INTERFOR. It aims at promoting comparability and synchronization of national procedures and harmonizing evaluation mechanisms through various methods including surveys and interviews (WP3).\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "Query 6: Tiempo = 167.0177 s\n",
      "La respuesta es: The European Joint Programme on Radioactive Waste Management (EURAD) was framed within Directive 2011/70/Euratom. This directive concerns measures to ensure a high common level of network and information security across the EU, which also recognizes cybersecurity's relevance for European economy preparedness among main stakeholders in this field.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes. \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "Query 7: Tiempo = 254.9432 s\n",
      "La respuesta es: The grant agreement 814416 was framed within the legal basis H2020-EU.3.5.SOCIETAL CHALLENGES - Climate action, Environment, Resource Efficiency and Raw Materials under master call SC5-2014-2015 in Horizon 2020 projects funded by the European Union.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "Query 8: Tiempo = 224.8078 s\n",
      "La respuesta es: The project G9NIGHT was submitted to master call H2020-MSCA-IF-2020.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " Yes \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      " No \n",
      "--------\n",
      "\n",
      "Query 9: Tiempo = 294.2231 s\n",
      "La respuesta es: The Electron Nanocrystallography project was submitted to subcall H2020-MSCA-IF-2020 within master call H2020-MSCA-IF-2018.\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    inicio = time.time()\n",
    "    current_query = eval(f\"query_{i}\")\n",
    "    filtered_docs = []\n",
    "    all_docs = []\n",
    "    def format_docs(docs):\n",
    "        all_docs.clear()\n",
    "        all_docs.extend(docs) \n",
    "        \n",
    "        filtered_docs.clear() \n",
    "        for doc in docs:\n",
    "            response = filter_chain.invoke({\n",
    "                \"question\": current_query,\n",
    "                \"document\": doc.page_content\n",
    "            })\n",
    "            respuesta_texto = response[\"text\"]\n",
    "            print(f\"\\n--------\\n {respuesta_texto} \\n--------\\n\")\n",
    "            if respuesta_texto.lower().startswith((\"sí\", \"yes\")):\n",
    "                filtered_docs.append(doc)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "\n",
    "    rag_chain_multi_7_txt = (\n",
    "        {\n",
    "            \"context\": retriever_multi_7_txt | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | phi_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Ejecutar RAG con los docs filtrados\n",
    "    answer = rag_chain_multi_7_txt.invoke(current_query)\n",
    "    fin = time.time()\n",
    "    tiempo_ejecucion = fin - inicio\n",
    "\n",
    "    # Almacenar todos los datos en el diccionario\n",
    "    resultados[f\"query_{i}\"] = {\n",
    "        \"query\": current_query,\n",
    "        \"answer\": answer,\n",
    "        \"all_docs\": all_docs,\n",
    "        \"filtered_docs\": filtered_docs\n",
    "    }\n",
    "\n",
    "    print(f\"Query {i}: Tiempo = {tiempo_ejecucion:.4f} s\")\n",
    "    print(f\"La respuesta es: {answer}\\n\\n---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a08245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación para Query 1 ---\n",
      "Pregunta: What is the objective of the project with grant agreement 740934?\n",
      "Respuesta correcta: The project’s objective is to combat violent extremism by analyzing its root causes, developing preventive and repressive measures, and countering extremist narratives through collaboration with civil society and LEAs, all while upholding fundamental rights.\n",
      "Respuesta generada: The objective of grant agreement 740934 under Horizon 2020 was to provide a conceptual basis for smart provision of public goods by EU agriculture and forestry ecosystems in the context of intensification scenarios. It aimed at offering tools, evidence, policy options, and improved incentives through transdisciplinary approaches while considering legislation impact across multiple scales within thirteen European countries.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 2 ---\n",
      "Pregunta: What is the total cost of the project with the acronym HYPERGRYD?\n",
      "Respuesta correcta: The total cost of the project with the acronym HYPERGRYD (grant agreement 101036656) was €5,987,875.00.\n",
      "Respuesta generada: The total funded cost for each project is explicitly mentioned next to its description, which allows us to directly answer your question without additional research or inference. The grant agreement number and the corresponding costs are as follows: HYPERGRYD (101036656) had a total funded cost of 5,987,874.5 euros; MSCA-IF project with gran agreement number 747921 also had a total funded cost of 183,454.8 euros (noting that this is the same amount as HYPERGRYD's); and finally, for grant agreement number 798271 under MSCA-IF with subcall H2020-MSCA-IF-2017, it had a total funded cost of 183,454.8 euros as well.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 3 ---\n",
      "Pregunta: How much funding was allocated for the project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration?\n",
      "Respuesta correcta: Transforming Research through Innovative Practices for Linked interdisciplinary Exploration” (TRIPLE), identified by grant agreement 863420, received a total EU contribution of € 5,626,548.75. This funding was allocated as part of Horizon 2020 under the “EXCELLENT SCIENCE – Research Infrastructures\n",
      "Respuesta generada: The project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration had a total funded cost of 137591.1 euros under grant agreement number 707706.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 4 ---\n",
      "Pregunta: Which organisation played the role of coordinator in the grant agreement 777998?\n",
      "Respuesta correcta: the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998.\n",
      "The organisation with Participant Identification Code (PIC) 960782479 participated in the grant agreement 777998. The name of this organisation is UNIVERSIDADE NOVA DE LISBOA. The organisation with PIC 960782479 is not a small or medium-sized enterprise. The organisation with PIC 960782479 develops an activity of type HES. The organisation with PIC 960782479 is based in the country PT, codified under ISO 3166. The organisation with PIC 960782479 played the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998 was 409500.0 euros. The total amount funded to the organization with PIC 960782479 in the grant agreement 777998 was 409500.0 euros.\n",
      "\n",
      "Respuesta generada: The European Commission (EC) acted as the project leader for Grant Agreement No. 777998 under Horizon 2020, while a consortium of partners from various countries took on coordinator roles to manage and execute the research activities effectively across borders.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 5 ---\n",
      "Pregunta: What topic does the project with the acronym INTERRFACE belong to?\n",
      "Respuesta correcta: The grant agreement 824330 was framed within the topic LC-SC3-ES-5-2018-2020TSO – DSO – Consumer: Large-scale demonstrations of innovative grid services through demand response, storage and small-scale (RES) generation. The grant agreement 824330 was framed within the master call H2020-LC-SC3-2018-2019-2020. The grant agreement 824330 was framed within the subcall H2020-LC-SC3-2018-ES-SCC.\n",
      "Respuesta generada: INTERRFACE is related to fostering dialogue with stakeholders for better forest protection within legal frameworks, as part of Horizon 2020 project INTERFOR. It aims at promoting comparability and synchronization of national procedures and harmonizing evaluation mechanisms through various methods including surveys and interviews (WP3).\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 6 ---\n",
      "Pregunta: What legal basis was the project titled European Joint Programme on Radioactive Waste Management framed within?\n",
      "Respuesta correcta: The grant agreement 847593 was framed within the legal basis H2020-EuratomEuratom.\n",
      "Respuesta generada: The European Joint Programme on Radioactive Waste Management (EURAD) was framed within Directive 2011/70/Euratom. This directive concerns measures to ensure a high common level of network and information security across the EU, which also recognizes cybersecurity's relevance for European economy preparedness among main stakeholders in this field.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 7 ---\n",
      "Pregunta: What type of proposal was the grant agreement 814416?\n",
      "Respuesta correcta: The grant agreement 814416 was a Research and Innovation Action (RIA) proposal.\n",
      "Respuesta generada: The grant agreement 814416 was framed within the legal basis H2020-EU.3.5.SOCIETAL CHALLENGES - Climate action, Environment, Resource Efficiency and Raw Materials under master call SC5-2014-2015 in Horizon 2020 projects funded by the European Union.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación para Query 8 ---\n",
      "Pregunta: To which master call was the project with the acronym G9NIGHT submitted?\n",
      "Respuesta correcta: The grant agreement 101036041 was framed within the master call H2020-MSCA-NIGHT-2020bis.\n",
      "Respuesta generada: The project G9NIGHT was submitted to master call H2020-MSCA-IF-2020.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación para Query 9 ---\n",
      "Pregunta: To which sub call was the project titled Electron Nanocrystallography submitted?\n",
      "Respuesta correcta: The grant agreement 956099 was framed within the legal basis H2020-EU.1.3.EXCELLENT SCIENCE - Marie Skłodowska-Curie Actions.\n",
      "Respuesta generada: The Electron Nanocrystallography project was submitted to subcall H2020-MSCA-IF-2020 within master call H2020-MSCA-IF-2018.\n",
      "Evaluación del LLM:\n",
      "Justification:\n",
      "The generated answer is completely unrelated to the correct answer, as it discusses EU agriculture and forestry ecosystems in the context of Horizon 2020, whereas the correct answer focuses on combating violent extremism through collaboration with civil society and LEAs. The two objectives are distinct and unrelated.\n",
      "\n",
      "Score: 1\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluaciones = {}  # Diccionario para guardar los resultados de la evaluación\n",
    "num_queries=9\n",
    "for i in range(1, num_queries + 1):\n",
    "    # Obtener datos del diccionario `resultados`\n",
    "    question = resultados[f\"query_{i}\"][\"query\"]\n",
    "    answer = resultados[f\"query_{i}\"][\"answer\"]\n",
    "    answer_correct = eval(f\"answer_{i}_correct\")\n",
    "    prompt_evaluation=prompt_evaluation.format(question=question, answer=answer, answer_correct=answer_correct)\n",
    "    response = llama_llm.invoke(prompt_evaluation)\n",
    "    evaluaciones[f\"query_{i}\"] = response\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n--- Evaluación para Query {i} ---\")\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {answer_correct}\")\n",
    "    print(f\"Respuesta generada: {answer}\")\n",
    "    print(f\"Evaluación del LLM:\\n{response}\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87dfadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800fa698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación para Query 1 ---\n",
      "Pregunta: What is the objective of the project with grant agreement 740934?\n",
      "Respuesta correcta: The project’s objective is to combat violent extremism by analyzing its root causes, developing preventive and repressive measures, and countering extremist narratives through collaboration with civil society and LEAs, all while upholding fundamental rights.\n",
      "Respuesta generada: The objective of grant agreement 740934 under Horizon 2020 was to provide a conceptual basis for smart provision of public goods by EU agriculture and forestry ecosystems in the context of intensification scenarios. It aimed at offering tools, evidence, policy options, and improved incentives through transdisciplinary approaches while considering legislation impact across multiple scales within thirteen European countries.\n",
      "Evaluación del LLM:\n",
      "**Respuesta**\n",
      "\n",
      "**Justificación de la respuesta**\n",
      "\n",
      "La respuesta generada no se ajusta a la pregunta original. La respuesta correcta habla sobre combatir el extremismo violento, mientras que la respuesta generada se refiere a la \"provision smart\" de bienes públicos en el contexto de la agricultura y la forestación.\n",
      "\n",
      "**Score respuesta**\n",
      "\n",
      "4\n",
      "\n",
      "**Score contexto**\n",
      "\n",
      "8\n",
      "---\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'respuesta_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluación del LLM:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m score_respuesta \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Score respuesta**: (\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrespuesta_llm\u001b[49m)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m score_contexto \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**Score contexto**: (\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\"\u001b[39m, respuesta_llm)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Convertir a enteros\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'respuesta_llm' is not defined"
     ]
    }
   ],
   "source": [
    "evaluaciones_with_context = {}\n",
    "puntuaciones = {}  # Nuevo diccionario para almacenar los scores\n",
    "num_queries = 2\n",
    "\n",
    "for i in range(1, num_queries + 1):\n",
    "    question = resultados[f\"query_{i}\"][\"query\"]\n",
    "    answer = resultados[f\"query_{i}\"][\"answer\"]\n",
    "    answer_correct = eval(f\"answer_{i}_correct\")\n",
    "    context=format_docs(resultados[f\"query_{i}\"][\"filtered_docs\"])\n",
    "    prompt_filled = prompt_evaluation_with_context.format(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        answer_correct=answer_correct,\n",
    "        context=context\n",
    "    )\n",
    "    response = llama_llm.invoke(prompt_filled)\n",
    "    evaluaciones_with_context[f\"query_{i}\"] = response\n",
    "    \n",
    "    print(f\"\\n--- Evaluación para Query {i} ---\")\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {answer_correct}\")\n",
    "    print(f\"Respuesta generada: {answer}\")\n",
    "    print(f\"Evaluación del LLM:\\n{response}\")\n",
    "    print(\"---\\n\")\n",
    "    score_respuesta = re.search(r\"**Score respuesta**: (\\d+)\", response).group(1)\n",
    "    score_contexto = re.search(r\"**Score contexto**: (\\d+)\", response).group(1)\n",
    "\n",
    "    # Convertir a enteros\n",
    "    score_respuesta = int(score_respuesta)\n",
    "    score_contexto = int(score_contexto)\n",
    "\n",
    "    print(\"Score respuesta:\", score_respuesta)  # Output: 1\n",
    "    print(\"Score contexto:\", score_contexto)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09c1fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(response_text, keyword):\n",
    "    pattern = rf\"(?i)\\*{{0,3}}score[_ ]{re.escape(keyword)}\\*{{0,3}}\\s*[:=]\\s*([0-9]+)\"\n",
    "    \n",
    "    match = re.search(pattern, response_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise AttributeError(f\"Score for '{keyword}' not found in text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c7a65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación para Query 1 ---\n",
      "Pregunta: What is the objective of the project with grant agreement 740934?\n",
      "Respuesta correcta: The project’s objective is to combat violent extremism by analyzing its root causes, developing preventive and repressive measures, and countering extremist narratives through collaboration with civil society and LEAs, all while upholding fundamental rights.\n",
      "Respuesta generada: The objective of grant agreement 740934 under Horizon 2020 was to provide a conceptual basis for smart provision of public goods by EU agriculture and forestry ecosystems in the context of intensification scenarios. It aimed at offering tools, evidence, policy options, and improved incentives through transdisciplinary approaches while considering legislation impact across multiple scales within thirteen European countries.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer is completely unrelated to the correct answer and does not even mention the grant agreement 740934. It seems that the context provided was not used at all in generating the response.\n",
      "Score_response: 1\n",
      "Score_context: 2\n",
      "---\n",
      "\n",
      "Score respuesta: None\n",
      "Score contexto: None\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 2 ---\n",
      "Pregunta: What is the total cost of the project with the acronym HYPERGRYD?\n",
      "Respuesta correcta: The total cost of the project with the acronym HYPERGRYD (grant agreement 101036656) was €5,987,875.00.\n",
      "Respuesta generada: The total funded cost for each project is explicitly mentioned next to its description, which allows us to directly answer your question without additional research or inference. The grant agreement number and the corresponding costs are as follows: HYPERGRYD (101036656) had a total funded cost of 5,987,874.5 euros; MSCA-IF project with gran agreement number 747921 also had a total funded cost of 183,454.8 euros (noting that this is the same amount as HYPERGRYD's); and finally, for grant agreement number 798271 under MSCA-IF with subcall H2020-MSCA-IF-2017, it had a total funded cost of 183,454.8 euros as well.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer is partially correct but lacks specificity and accuracy. It incorrectly states that the total funded cost for HYPERGRYD is 5,987,874.5 euros, whereas the correct answer is 5,987,875.00 euros.\n",
      "Score response: 6\n",
      "Score context: 8\n",
      "---\n",
      "\n",
      "Score respuesta: 6\n",
      "Score contexto: 8\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 3 ---\n",
      "Pregunta: How much funding was allocated for the project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration?\n",
      "Respuesta correcta: Transforming Research through Innovative Practices for Linked interdisciplinary Exploration” (TRIPLE), identified by grant agreement 863420, received a total EU contribution of € 5,626,548.75. This funding was allocated as part of Horizon 2020 under the “EXCELLENT SCIENCE – Research Infrastructures\n",
      "Respuesta generada: The project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration had a total funded cost of 137591.1 euros under grant agreement number 707706.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer only partially matches the correct answer, as it mentions a total funded cost of 137591.1 euros under grant agreement number 707706, but does not provide the correct total EU contribution of € 5,626,548.75 for the project TRIPLE.\n",
      "Score response: 4\n",
      "Score context: 6\n",
      "---\n",
      "\n",
      "Score respuesta: 4\n",
      "Score contexto: 6\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 4 ---\n",
      "Pregunta: Which organisation played the role of coordinator in the grant agreement 777998?\n",
      "Respuesta correcta: the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998.\n",
      "The organisation with Participant Identification Code (PIC) 960782479 participated in the grant agreement 777998. The name of this organisation is UNIVERSIDADE NOVA DE LISBOA. The organisation with PIC 960782479 is not a small or medium-sized enterprise. The organisation with PIC 960782479 develops an activity of type HES. The organisation with PIC 960782479 is based in the country PT, codified under ISO 3166. The organisation with PIC 960782479 played the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998 was 409500.0 euros. The total amount funded to the organization with PIC 960782479 in the grant agreement 777998 was 409500.0 euros.\n",
      "\n",
      "Respuesta generada: The European Commission (EC) acted as the project leader for Grant Agreement No. 777998 under Horizon 2020, while a consortium of partners from various countries took on coordinator roles to manage and execute the research activities effectively across borders.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer incorrectly identifies the European Commission as the coordinator, whereas the correct information states that the organisation with PIC 960782479 played the role of coordinator. Additionally, the context does not mention the European Commission or Horizon 2020, which is unrelated to the question.\n",
      "Score response: 2\n",
      "Score context: 1\n",
      "---\n",
      "\n",
      "Score respuesta: 2\n",
      "Score contexto: 1\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 5 ---\n",
      "Pregunta: What topic does the project with the acronym INTERRFACE belong to?\n",
      "Respuesta correcta: The grant agreement 824330 was framed within the topic LC-SC3-ES-5-2018-2020TSO – DSO – Consumer: Large-scale demonstrations of innovative grid services through demand response, storage and small-scale (RES) generation. The grant agreement 824330 was framed within the master call H2020-LC-SC3-2018-2019-2020. The grant agreement 824330 was framed within the subcall H2020-LC-SC3-2018-ES-SCC.\n",
      "Respuesta generada: INTERRFACE is related to fostering dialogue with stakeholders for better forest protection within legal frameworks, as part of Horizon 2020 project INTERFOR. It aims at promoting comparability and synchronization of national procedures and harmonizing evaluation mechanisms through various methods including surveys and interviews (WP3).\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer is partially correct, but it does not accurately describe the project INTERRFACE. The correct answer is related to fostering dialogue with stakeholders for better forest protection within legal frameworks, as part of Horizon 2020 project INTERFOR.\n",
      "\n",
      "Score response: 4\n",
      "\n",
      "Response_justification: The context provided is highly relevant to the topic of INTERRFACE, as it describes the project's goals and objectives in detail. The context adequately supports the answer by providing information about the project's focus on fostering dialogue with stakeholders for better forest protection within legal frameworks.\n",
      "\n",
      "Score context: 9\n",
      "---\n",
      "\n",
      "Score respuesta: 4\n",
      "Score contexto: 9\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 6 ---\n",
      "Pregunta: What legal basis was the project titled European Joint Programme on Radioactive Waste Management framed within?\n",
      "Respuesta correcta: The grant agreement 847593 was framed within the legal basis H2020-EuratomEuratom.\n",
      "Respuesta generada: The European Joint Programme on Radioactive Waste Management (EURAD) was framed within Directive 2011/70/Euratom. This directive concerns measures to ensure a high common level of network and information security across the EU, which also recognizes cybersecurity's relevance for European economy preparedness among main stakeholders in this field.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer incorrectly states that Directive 2011/70/Euratom concerns measures to ensure a high common level of network and information security across the EU, which is not relevant to radioactive waste management. The correct legal basis for the European Joint Programme on Radioactive Waste Management is actually H2020-EuratomEuratom.\n",
      "Score response: 4\n",
      "Score context: 8\n",
      "---\n",
      "\n",
      "Score respuesta: 4\n",
      "Score contexto: 8\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 7 ---\n",
      "Pregunta: What type of proposal was the grant agreement 814416?\n",
      "Respuesta correcta: The grant agreement 814416 was a Research and Innovation Action (RIA) proposal.\n",
      "Respuesta generada: The grant agreement 814416 was framed within the legal basis H2020-EU.3.5.SOCIETAL CHALLENGES - Climate action, Environment, Resource Efficiency and Raw Materials under master call SC5-2014-2015 in Horizon 2020 projects funded by the European Union.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The grant agreement 814416 was framed within the legal basis H2020-EU.3.5.SOCIETAL CHALLENGES - Climate action, Environment, Resource Efficiency and Raw Materials under master call SC5-2014-2015 in Horizon 2020 projects funded by the European Union.\n",
      "Score response: 2\n",
      "Score context: 8\n",
      "---\n",
      "\n",
      "Score respuesta: 2\n",
      "Score contexto: 8\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación para Query 8 ---\n",
      "Pregunta: To which master call was the project with the acronym G9NIGHT submitted?\n",
      "Respuesta correcta: The grant agreement 101036041 was framed within the master call H2020-MSCA-NIGHT-2020bis.\n",
      "Respuesta generada: The project G9NIGHT was submitted to master call H2020-MSCA-IF-2020.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer is partially correct, as it mentions the master call H2020-MSCA-IF-2020, but incorrectly includes \"H2020-MSCA-NIGHT-2020bis\" which is not present in the provided data. The correct master call for G9NIGHT is actually H2020-MSCA-NIGHT-2020bis.\n",
      "Score response: 6\n",
      "Score context: 8\n",
      "---\n",
      "\n",
      "Score respuesta: 6\n",
      "Score contexto: 8\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluación para Query 9 ---\n",
      "Pregunta: To which sub call was the project titled Electron Nanocrystallography submitted?\n",
      "Respuesta correcta: The grant agreement 956099 was framed within the legal basis H2020-EU.1.3.EXCELLENT SCIENCE - Marie Skłodowska-Curie Actions.\n",
      "Respuesta generada: The Electron Nanocrystallography project was submitted to subcall H2020-MSCA-IF-2020 within master call H2020-MSCA-IF-2018.\n",
      "Evaluación del LLM:\n",
      "Response_justification: The generated answer is partially correct, but it incorrectly states that the project was submitted to subcall H2020-MSCA-IF-2018. The correct subcall is H2020-MSCA-IF-2020.\n",
      "Score response: 6\n",
      "Score context: 9\n",
      "---\n",
      "\n",
      "Score respuesta: 6\n",
      "Score contexto: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluaciones_with_context = {}\n",
    "puntuaciones = {} \n",
    "num_queries = 9\n",
    "\n",
    "for i in range(1, num_queries + 1):\n",
    "    question = resultados[f\"query_{i}\"][\"query\"]\n",
    "    answer = resultados[f\"query_{i}\"][\"answer\"]\n",
    "    answer_correct = eval(f\"answer_{i}_correct\")\n",
    "    context = format_docs(resultados[f\"query_{i}\"][\"filtered_docs\"])\n",
    "    \n",
    "    prompt_filled = prompt_evaluation_with_context.format(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        answer_correct=answer_correct,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    response = llama_llm.invoke(prompt_filled)\n",
    "    evaluaciones_with_context[f\"query_{i}\"] = response\n",
    "    \n",
    "    print(f\"\\n--- Evaluación para Query {i} ---\")\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {answer_correct}\")\n",
    "    print(f\"Respuesta generada: {answer}\")\n",
    "    print(f\"Evaluación del LLM:\\n{response}\")\n",
    "    print(\"---\\n\")\n",
    "    \n",
    "    # Extraer scores de la respuesta del LLM (response, no respuesta_llm)\n",
    "    try:\n",
    "        score_respuesta = extract_score(response, \"response\")\n",
    "        score_contexto = extract_score(response, \"context\")\n",
    "    except AttributeError:\n",
    "        print(f\"Error: No se encontraron scores en la respuesta para Query {i}\")\n",
    "        score_respuesta = None\n",
    "        score_contexto = None\n",
    "    \n",
    "    # Guardar en el diccionario de puntuaciones\n",
    "    puntuaciones[f\"query_{i}\"] = {\n",
    "        \"score_respuesta\": score_respuesta,\n",
    "        \"score_contexto\": score_contexto\n",
    "    }\n",
    "    \n",
    "    print(\"Score respuesta:\", score_respuesta)\n",
    "    print(\"Score contexto:\", score_contexto)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6abfec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nota media de las respuestas: 0.425\n",
      "Nota media de los contextos seleccionados: 0.713\n"
     ]
    }
   ],
   "source": [
    "scores_respuesta = [v['score_respuesta'] for v in puntuaciones.values() if v['score_respuesta'] is not None]\n",
    "scores_contexto = [v['score_contexto'] for v in puntuaciones.values() if v['score_contexto'] is not None]\n",
    "\n",
    "max_score = 10\n",
    "scores_respuesta_norm = [score / max_score for score in scores_respuesta] \n",
    "scores_contexto_norm = [score / max_score for score in scores_contexto]   \n",
    "\n",
    "# Calcular métricas\n",
    "mean_respuesta = np.mean(scores_respuesta_norm) \n",
    "mean_contexto = np.mean(scores_contexto_norm)\n",
    "\n",
    "print(f\"Nota media de las respuestas: {mean_respuesta:.3f}\")\n",
    "print(f\"Nota media de los contextos seleccionados: {mean_contexto:.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fbf609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación extendida para Query 1 ---\n",
      "Pregunta: What is the objective of the project with grant agreement 740934?\n",
      "Respuesta correcta: The project’s objective is to combat violent extremism by analyzing its root causes, developing preventive and repressive measures, and countering extremist narratives through collaboration with civil society and LEAs, all while upholding fundamental rights.\n",
      "Respuesta generada: The objective of grant agreement 740934 under Horizon 2020 was to provide a conceptual basis for smart provision of public goods by EU agriculture and forestry ecosystems in the context of intensification scenarios. It aimed at offering tools, evidence, policy options, and improved incentives through transdisciplinary approaches while considering legislation impact across multiple scales within thirteen European countries.\n",
      "Evaluación del LLM:\n",
      "Justification: The generated answer is not fully supported by the retrieved context, as it mentions a different project objective and scope than the correct answer.\n",
      "\n",
      "Score_faithfulness: 2\n",
      "Score_coverage: 4\n",
      "Score_fluency: 6\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 2 ---\n",
      "Pregunta: What is the total cost of the project with the acronym HYPERGRYD?\n",
      "Respuesta correcta: The total cost of the project with the acronym HYPERGRYD (grant agreement 101036656) was €5,987,875.00.\n",
      "Respuesta generada: The total funded cost for each project is explicitly mentioned next to its description, which allows us to directly answer your question without additional research or inference. The grant agreement number and the corresponding costs are as follows: HYPERGRYD (101036656) had a total funded cost of 5,987,874.5 euros; MSCA-IF project with gran agreement number 747921 also had a total funded cost of 183,454.8 euros (noting that this is the same amount as HYPERGRYD's); and finally, for grant agreement number 798271 under MSCA-IF with subcall H2020-MSCA-IF-2017, it had a total funded cost of 183,454.8 euros as well.\n",
      "Evaluación del LLM:\n",
      "Justification: The generated answer is partially faithful to the retrieved context, as it mentions some of the grant agreements and their costs. However, it also introduces incorrect information about the total cost of HYPERGRYD (€5,987,874.5) and incorrectly states that MSCA-IF project with gran agreement number 747921 had a total funded cost of €183,454.8 euros.\n",
      "\n",
      "Score_faithfulness: 4\n",
      "\n",
      "Justification: The retrieved context does not contain enough information to correctly answer the question about the total cost of the HYPERGRYD project.\n",
      "\n",
      "Score_coverage: 2\n",
      "\n",
      "Justification: The generated answer is clear and well-written, but it lacks coherence and logical flow. It jumps between different grant agreements and their costs without a clear connection or explanation.\n",
      "\n",
      "Score_fluency: 6\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 3 ---\n",
      "Pregunta: How much funding was allocated for the project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration?\n",
      "Respuesta correcta: Transforming Research through Innovative Practices for Linked interdisciplinary Exploration” (TRIPLE), identified by grant agreement 863420, received a total EU contribution of € 5,626,548.75. This funding was allocated as part of Horizon 2020 under the “EXCELLENT SCIENCE – Research Infrastructures\n",
      "Respuesta generada: The project titled Transforming Research through Innovative Practices for Linked interdisciplinary Exploration had a total funded cost of 137591.1 euros under grant agreement number 707706.\n",
      "Evaluación del LLM:\n",
      "Score_faithfulness: 2\n",
      "Score_coverage: 6\n",
      "Score_fluency: 4\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 4 ---\n",
      "Pregunta: Which organisation played the role of coordinator in the grant agreement 777998?\n",
      "Respuesta correcta: the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998.\n",
      "The organisation with Participant Identification Code (PIC) 960782479 participated in the grant agreement 777998. The name of this organisation is UNIVERSIDADE NOVA DE LISBOA. The organisation with PIC 960782479 is not a small or medium-sized enterprise. The organisation with PIC 960782479 develops an activity of type HES. The organisation with PIC 960782479 is based in the country PT, codified under ISO 3166. The organisation with PIC 960782479 played the role of coordinator in the grant agreement 777998. The participation cost of the organisation with PIC 960782479 in the grant agreement 777998 was 409500.0 euros. The total amount funded to the organization with PIC 960782479 in the grant agreement 777998 was 409500.0 euros.\n",
      "\n",
      "Respuesta generada: The European Commission (EC) acted as the project leader for Grant Agreement No. 777998 under Horizon 2020, while a consortium of partners from various countries took on coordinator roles to manage and execute the research activities effectively across borders.\n",
      "Evaluación del LLM:\n",
      "Score_faithfulness: 2\n",
      "Score_coverage: 8\n",
      "Score_fluency: 4\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 5 ---\n",
      "Pregunta: What topic does the project with the acronym INTERRFACE belong to?\n",
      "Respuesta correcta: The grant agreement 824330 was framed within the topic LC-SC3-ES-5-2018-2020TSO – DSO – Consumer: Large-scale demonstrations of innovative grid services through demand response, storage and small-scale (RES) generation. The grant agreement 824330 was framed within the master call H2020-LC-SC3-2018-2019-2020. The grant agreement 824330 was framed within the subcall H2020-LC-SC3-2018-ES-SCC.\n",
      "Respuesta generada: INTERRFACE is related to fostering dialogue with stakeholders for better forest protection within legal frameworks, as part of Horizon 2020 project INTERFOR. It aims at promoting comparability and synchronization of national procedures and harmonizing evaluation mechanisms through various methods including surveys and interviews (WP3).\n",
      "Evaluación del LLM:\n",
      "Justification: The generated answer was not fully supported by the retrieved context, as it introduced new information (fostering dialogue with stakeholders for better forest protection) that is not present in the provided context.\n",
      "\n",
      "Score_faithfulness: 4\n",
      "Score_coverage: 6\n",
      "Score_fluency: 7\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 6 ---\n",
      "Pregunta: What legal basis was the project titled European Joint Programme on Radioactive Waste Management framed within?\n",
      "Respuesta correcta: The grant agreement 847593 was framed within the legal basis H2020-EuratomEuratom.\n",
      "Respuesta generada: The European Joint Programme on Radioactive Waste Management (EURAD) was framed within Directive 2011/70/Euratom. This directive concerns measures to ensure a high common level of network and information security across the EU, which also recognizes cybersecurity's relevance for European economy preparedness among main stakeholders in this field.\n",
      "Evaluación del LLM:\n",
      "Justification: The generated answer partially misrepresents the context, incorrectly stating that Directive 2011/70/Euratom concerns measures for a high common level of network and information security across the EU. However, the correct context indicates that this directive recognizes cybersecurity's relevance for European economy preparedness among main stakeholders.\n",
      "\n",
      "Score_faithfulness: 4\n",
      "Score_coverage: 6\n",
      "Score_fluency: 8\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 7 ---\n",
      "Pregunta: What type of proposal was the grant agreement 814416?\n",
      "Respuesta correcta: The grant agreement 814416 was a Research and Innovation Action (RIA) proposal.\n",
      "Respuesta generada: The grant agreement 814416 was framed within the legal basis H2020-EU.3.5.SOCIETAL CHALLENGES - Climate action, Environment, Resource Efficiency and Raw Materials under master call SC5-2014-2015 in Horizon 2020 projects funded by the European Union.\n",
      "Evaluación del LLM:\n",
      "Here is the evaluation in the STRICT OUTPUT FORMAT:\n",
      "\n",
      "Justification:\n",
      "The generated answer partially aligns with the context, but it introduces additional information not present in the provided data. The correct answer is supported by the context, which mentions the grant agreement 641816 being framed within the topic SC5-01-2014Advanced Earth-system models.\n",
      "\n",
      "Score_faithfulness: 6\n",
      "Score_coverage: 8\n",
      "Score_fluency: 7\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación extendida para Query 8 ---\n",
      "Pregunta: To which master call was the project with the acronym G9NIGHT submitted?\n",
      "Respuesta correcta: The grant agreement 101036041 was framed within the master call H2020-MSCA-NIGHT-2020bis.\n",
      "Respuesta generada: The project G9NIGHT was submitted to master call H2020-MSCA-IF-2020.\n",
      "Evaluación del LLM:\n",
      "Score_faithfulness: 6\n",
      "Score_coverage: 8\n",
      "Score_fluency: 4\n",
      "---\n",
      "\n",
      "\n",
      "--- Evaluación extendida para Query 9 ---\n",
      "Pregunta: To which sub call was the project titled Electron Nanocrystallography submitted?\n",
      "Respuesta correcta: The grant agreement 956099 was framed within the legal basis H2020-EU.1.3.EXCELLENT SCIENCE - Marie Skłodowska-Curie Actions.\n",
      "Respuesta generada: The Electron Nanocrystallography project was submitted to subcall H2020-MSCA-IF-2020 within master call H2020-MSCA-IF-2018.\n",
      "Evaluación del LLM:\n",
      "Justification: The generated answer was partially supported by the retrieved context, as it mentioned the correct subcall (H2020-MSCA-IF-2020) but incorrectly stated the master call (H2020-MSCA-IF-2018).\n",
      "\n",
      "Score_faithfulness: 6\n",
      "Score_coverage: 9\n",
      "Score_fluency: 7\n",
      "---\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'query_10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m num_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_queries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[43mresultados\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m     answer \u001b[38;5;241m=\u001b[39m resultados[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     35\u001b[0m     answer_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_correct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query_10'"
     ]
    }
   ],
   "source": [
    "prompt_evaluation_extended = \"\"\"\n",
    "You are an advanced evaluator. You must evaluate:\n",
    "\n",
    "1. Faithfulness: Is the generated answer fully supported by the retrieved context? (Score 1–10)\n",
    "2. Coverage: Does the retrieved context contain enough information to correctly answer the question? (Score 1–10)\n",
    "3. Fluency: Is the generated answer clear, coherent, and well-written? (Score 1–10)\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Read the question, the correct answer, the generated answer, and the context.\n",
    "- Assign three scores between 1 and 10.\n",
    "\n",
    "DATA:\n",
    "Question: {question}\n",
    "Correct answer: {answer_correct}\n",
    "Generated answer: {answer}\n",
    "Context: {context}\n",
    "\n",
    "STRICT OUTPUT FORMAT:\n",
    "Justification: [Concise explanation]\n",
    "Score_faithfulness: [1–10]\n",
    "Score_coverage: [1–10]\n",
    "Score_fluency: [1–10]\n",
    "\n",
    "The output has to be ONLY THE STRICT OUTPUT FORMAT.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "evaluaciones_extended = {}\n",
    "puntuaciones_extended = {}\n",
    "num_queries = 10\n",
    "\n",
    "for i in range(1, num_queries + 1):\n",
    "    question = resultados[f\"query_{i}\"][\"query\"]\n",
    "    answer = resultados[f\"query_{i}\"][\"answer\"]\n",
    "    answer_correct = eval(f\"answer_{i}_correct\")\n",
    "    context = format_docs(resultados[f\"query_{i}\"][\"filtered_docs\"])\n",
    "    \n",
    "    prompt_filled = prompt_evaluation_extended.format(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        answer_correct=answer_correct,\n",
    "        context=context\n",
    "    )\n",
    "    response = llama_llm.invoke(prompt_filled)\n",
    "    evaluaciones_extended[f\"query_{i}\"] = response\n",
    "    \n",
    "    print(f\"\\n--- Evaluación extendida para Query {i} ---\")\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {answer_correct}\")\n",
    "    print(f\"Respuesta generada: {answer}\")\n",
    "    print(f\"Evaluación del LLM:\\n{response}\")\n",
    "    print(\"---\\n\")\n",
    "    \n",
    "    try:\n",
    "        score_faithfulness = extract_score(response, \"faithfulness\")\n",
    "        score_coverage = extract_score(response, \"coverage\")\n",
    "        score_fluency = extract_score(response, \"fluency\")\n",
    "    except AttributeError:\n",
    "        print(f\"Error: No se encontraron scores en la respuesta para Query {i}\")\n",
    "        score_faithfulness = None\n",
    "        score_coverage = None\n",
    "        score_fluency = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64c3be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_global_score(score_faithfulness, score_coverage, score_fluency):\n",
    "    \"\"\"\n",
    "    Calcula el global_score a partir de los tres scores.\n",
    "    Devuelve un diccionario con todos los scores y el global.\n",
    "    \"\"\"\n",
    "    if None not in (score_faithfulness, score_coverage, score_fluency):\n",
    "        global_score = (\n",
    "            0.4 * score_faithfulness +\n",
    "            0.4 * score_coverage +\n",
    "            0.2 * score_fluency\n",
    "        )\n",
    "    else:\n",
    "        global_score = None\n",
    "    \n",
    "    return {\n",
    "        \"score_faithfulness\": score_faithfulness,\n",
    "        \"score_coverage\": score_coverage,\n",
    "        \"score_fluency\": score_fluency,\n",
    "        \"global_score\": global_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fe21169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score faithfulness: 6\n",
      "Score coverage: 9\n",
      "Score fluency: 7\n",
      "Global score: 7.4\n"
     ]
    }
   ],
   "source": [
    "scores_dict = calcular_global_score(score_faithfulness, score_coverage, score_fluency)\n",
    "\n",
    "puntuaciones_extended[f\"query_{i}\"] = scores_dict\n",
    "\n",
    "\n",
    "print(\"Score faithfulness:\", scores_dict[\"score_faithfulness\"])\n",
    "print(\"Score coverage:\", scores_dict[\"score_coverage\"])\n",
    "print(\"Score fluency:\", scores_dict[\"score_fluency\"])\n",
    "print(\"Global score:\", scores_dict[\"global_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53754010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 REAL",
   "language": "python",
   "name": "py310real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
