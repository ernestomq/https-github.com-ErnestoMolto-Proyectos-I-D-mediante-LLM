{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4602b529",
   "metadata": {},
   "source": [
    "# METRICAS PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee81eb",
   "metadata": {},
   "source": [
    "## APIPE (Average Precision in Exact Passage Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks=0\n",
    "ground_truth_field=0 ## Información que buscamos como ID o el precio\n",
    "\n",
    "hits = [1 if ground_truth_field in chunk else 0 for chunk in retrieved_chunks]\n",
    "precision = sum(hits) / len(hits) if hits else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54af93",
   "metadata": {},
   "source": [
    "## APKAC (Average Precision in K-chunk Answer Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks=0\n",
    "supporting_context=0 # fragmento donde se encuentra la respuesta\n",
    "\n",
    "hits = [1 if supporting_context in chunk else 0 for chunk in retrieved_chunks]\n",
    "precision = sum(hits) / len(hits) if hits else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1002783",
   "metadata": {},
   "source": [
    "# Evaluación Automática con LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_from_response(response_text):\n",
    "    match = re.search(r\"[Pp]untuaci[oó]n\\s*[:=]?\\s*(\\d)\", response_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f999553",
   "metadata": {},
   "source": [
    "## Factual Consistency Score (FCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "num_queries = 25\n",
    "result={}\n",
    "scores={}    \n",
    "for i in range(1, num_queries + 1):\n",
    "    question = f\"query_{i}\"\n",
    "    answer=f\"answer_{i}\"\n",
    "    context=f\"retrieved_chunks_{i}\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Eres un evaluador experto. Vas a evaluar si una respuesta está completamente justificada por un contexto.\n",
    "\n",
    "    Pregunta: {question}\n",
    "\n",
    "    Contexto recuperado:\n",
    "    {context}\n",
    "\n",
    "    Respuesta generada por el sistema:\n",
    "    {answer}\n",
    "\n",
    "    ¿Está la respuesta completamente basada en el contexto? Evalúa del 1 al 5, donde:\n",
    "    1 = inventada completamente,\n",
    "    5 = totalmente basada en el contexto.\n",
    "\n",
    "    Explica brevemente tu decisión y da la puntuación al final.\n",
    "\n",
    "    Formato de salida:\n",
    "    Justificación: ...\n",
    "    Puntuación: <número del 1 al 5>\n",
    "    \"\"\"\n",
    "    \n",
    "    results[i] = llm.invoke(prompt)  # o llm(prompt)\n",
    "    scores[i] = extract_score_from_response(results[i])\n",
    "\n",
    "    print(f\"Puntuación consulta {i}: {scores[i]}\")\n",
    "    print(f\"Explicación de LLaMA:\\n {results[i]}\")\n",
    "    print(\"\\n\\n---\\n\\n\")\n",
    "    \n",
    "    rows.append({\n",
    "        \"query_id\": i,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_chunks\": context,\n",
    "        \"llm_response\": response,\n",
    "        \"score\": score\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389cf94",
   "metadata": {},
   "source": [
    "Crear el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"evaluacion_rag_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b41e92",
   "metadata": {},
   "source": [
    "Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64317018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histograma\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[\"score\"], bins=5, kde=True)\n",
    "plt.title(\"Distribución de Puntuaciones (Histograma)\")\n",
    "plt.xlabel(\"Puntuación\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "# Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=df[\"score\"])\n",
    "plt.title(\"Boxplot de Puntuaciones\")\n",
    "plt.xlabel(\"Puntuación\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90e119",
   "metadata": {},
   "source": [
    "## Sub-question Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b67e7e",
   "metadata": {},
   "source": [
    "Voy a utilizar las consultas generadas a través de multiquery y a partir de estas realizar está metrica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbc4a0",
   "metadata": {},
   "source": [
    "Este codigo puedo tambien añadirlo antes de las subconsultas del multiquery haciendo una evaluación como la de antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_from_response(response_text):\n",
    "    match = re.search(r\"[Pp]untuaci[oó]n\\s*[:=]?\\s*(\\d)\", response_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "num_queries = 25\n",
    "result={}\n",
    "scores={}    \n",
    "for i in range(1, num_queries + 1):\n",
    "    \n",
    "    question = f\"query_{i}\"\n",
    "    retrieved_docs = retriever_multi_7_txt.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) # Se puede limitar el contexto\n",
    "    answer = rag_chain_multi_7_txt.invoke(query)\n",
    "    \n",
    "    sub_questions = retriever_multi_7_txt.generate_queries(query, 3)\n",
    "    sub_evals = {}\n",
    "    for i, sub_q in enumerate(sub_questions):\n",
    "        prompt = f\"\"\"Evalúa del 1 al 5 si esta respuesta responde a la pregunta usando SOLO el contexto:\n",
    "\n",
    "        Pregunta: {question}\n",
    "        Contexto: {context}\n",
    "        Respuesta: {answer}\n",
    "\n",
    "        Devuelve SOLO el número (1=inventada, 5=exacta):\"\"\"\n",
    "    \n",
    "        eval_result[i] = llm.invoke(prompt)\n",
    "        sub_evals[i] = extract_score_from_response(eval_result[i])      \n",
    "        print(f\"La consulta {i} tiene las subconsulta {sub_q} con una nota de {sub_evals}\")\n",
    "        \n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 REAL",
   "language": "python",
   "name": "py310real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
