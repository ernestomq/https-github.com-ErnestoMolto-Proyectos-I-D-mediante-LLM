{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0550f1-c2c5-41ef-bc3d-f9845c3c3ede",
   "metadata": {},
   "source": [
    "# Crear bases de vectores de Chroma: 1, 2 ,3, 4, 5 y 6\n",
    "## Cada vez que se vaya a crear una nueva base de vectores, es mejor reiniciar el kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8001c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-community\n",
      "Version: 0.2.19\n",
      "Summary: Community contributed LangChain integrations.\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: MIT\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: SQLAlchemy, requests, tenacity, langsmith, langchain-core, numpy, PyYAML, aiohttp, langchain, dataclasses-json\n",
      "Required-by: \n",
      "Name: chromadb\n",
      "Version: 0.5.23\n",
      "Summary: Chroma.\n",
      "Home-page: None\n",
      "Author: None\n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: None\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: uvicorn, tokenizers, build, tqdm, opentelemetry-exporter-otlp-proto-grpc, pydantic, mmh3, chroma-hnswlib, kubernetes, onnxruntime, typer, orjson, typing-extensions, fastapi, posthog, opentelemetry-instrumentation-fastapi, opentelemetry-api, rich, bcrypt, numpy, overrides, grpcio, importlib-resources, opentelemetry-sdk, PyYAML, httpx, pypika, tenacity, graphlib-backport\n",
      "Required-by: langchain-chroma\n",
      "Name: gpt4all\n",
      "Version: 2.8.2\n",
      "Summary: Python bindings for GPT4All\n",
      "Home-page: https://gpt4all.io/\n",
      "Author: Nomic and the Open Source Community\n",
      "Author-email: support@nomic.ai\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: importlib-resources, tqdm, requests\n",
      "Required-by: \n",
      "Name: langchain-text-splitters\n",
      "Version: 0.2.4\n",
      "Summary: LangChain text splitting utilities\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: MIT\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: langchain-core\n",
      "Required-by: langchain\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-community\n",
    "!pip show chromadb\n",
    "!pip show gpt4all\n",
    "!pip show langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8adf74b-5847-419a-bed4-90e52ddb41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647cf36-fc50-4b12-952e-09f7188466d0",
   "metadata": {},
   "source": [
    "### Se cargan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b10498-08f2-4a34-9f4c-3d4bb9e88a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA CREAR METADATOS\n",
    "import csv\n",
    "from typing import Dict, List, Optional\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "class CSVLoader(BaseLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        source_column: Optional[str] = None,\n",
    "        metadata_columns: Optional[List[str]] = None,\n",
    "        csv_args: Optional[Dict] = None,\n",
    "        encoding: Optional[str] = None,\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.source_column = source_column\n",
    "        self.encoding = encoding\n",
    "        self.csv_args = csv_args or {}\n",
    "        self.metadata_columns = metadata_columns or []\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        docs = []\n",
    "        with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
    "            csv_reader = csv.DictReader(csvfile, **self.csv_args)\n",
    "            for i, row in enumerate(csv_reader):\n",
    "                metadata = {\"row\": i}\n",
    "                for col in self.metadata_columns:\n",
    "                    if col in row:\n",
    "                        metadata[col] = row[col].strip()\n",
    "                content = []\n",
    "                for k, v in row.items():\n",
    "                    if k != self.source_column and k not in self.metadata_columns:\n",
    "                        content.append(f\"{k.strip()}: {v.strip()}\")\n",
    "                doc_content = \"\\n\".join(content)\n",
    "                doc = Document(page_content=doc_content, metadata=metadata)\n",
    "                docs.append(doc)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9be0d5-4f73-4891-b62a-6c55eed986aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = [\"row\"]\n",
    "\n",
    "# Instancia el CSVLoader con el archivo CSV y las columnas de metadatos\n",
    "loader = CSVLoader(\n",
    "    file_path = r\"C:\\Users\\emolt\\OneDrive - UMH\\MASTER\\TFM\\BASE\\cordis_data_processed.csv\",\n",
    "    source_column= None,  # Opcional: columna para establecer como origen\n",
    "    metadata_columns=metadata_columns,\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "# Carga los documentos del CSV\n",
    "raw_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf2d66-ac63-42d5-a94a-5645b4ccc1a1",
   "metadata": {},
   "source": [
    "## Chroma_db_1, Chroma_db_2, Chroma_db_3: mismos chunks (1000), distintos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc49507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-text-splitters\n",
      "Version: 0.2.4\n",
      "Summary: LangChain text splitting utilities\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: MIT\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: langchain-core\n",
      "Required-by: langchain\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d23e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['langchain', 'langchain_community', 'langchain_core', 'langchain_ollama', 'langchain_text_splitters']\n"
     ]
    }
   ],
   "source": [
    "import pkgutil\n",
    "print([name for _, name, _ in pkgutil.iter_modules() if \"langchain\" in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7422f9a5-e480-4043-af99-c37c087b07a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=800,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(raw_documents[0:100])\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9088835-8ff7-4837-9f24-c5bd18a00a9a",
   "metadata": {},
   "source": [
    "### Chroma_db_1: GPT4ALLEmbeddings (22,7M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24ee9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gpt4all\n",
      "Version: 2.8.2\n",
      "Summary: Python bindings for GPT4All\n",
      "Home-page: https://gpt4all.io/\n",
      "Author: Nomic and the Open Source Community\n",
      "Author-email: support@nomic.ai\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\emolt\\anaconda3\\lib\\site-packages\n",
      "Requires: importlib-resources, requests, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9d7179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\anaconda3\\envs\\py310real\\python.exe\n",
      "['C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\python310.zip', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\DLLs', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\lib', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real', '', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\lib\\\\site-packages', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\emolt\\\\anaconda3\\\\envs\\\\py310real\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Should point to your py310real environment\n",
    "print(sys.path)  # Should include your env's site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1a7a64-6bc6-4399-8904-92d5af1056de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "emb = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a9d8d1-a49e-4292-9f69-9a86e5cf3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para crear la base de vectores\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef99217-524e-4ad4-99c8-2a09c1d66903",
   "metadata": {},
   "source": [
    "### Chroma_db_2: bge-large-en (335M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f76858dc-95b6-4f9f-9d53-5a3c586ff927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_21536\\2112104021.py:9: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
      "C:\\Users\\emolt\\anaconda3\\envs\\py310real\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# SE USA UNA DE LAS GPU\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # Al no tener NVIDIA es necesario cambiarlo model_kwargs = {'device': 'cuda:0'}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162b23f4-8dd2-44e4-ab70-c2425ea71811",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701491f-7ffb-45cf-a70e-58048db97c5f",
   "metadata": {},
   "source": [
    "### Chroma_db_3: all-mpnet-base-v2 (109M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487ec048-2dc0-496b-8908-68f03c106fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emolt\\AppData\\Local\\Temp\\ipykernel_21536\\2067788043.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# SE USA UNA DE LAS GPU\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}  # specify GPU device\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1752b47-cdc0-4d99-8e0c-2bfd9a00b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8525f-200b-4e19-9e4a-660bbf523f12",
   "metadata": {},
   "source": [
    "## Chroma_db_4, Chroma_db_5, Chroma_db_5: mismos chunks (500), distintos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca5a291-529c-4ac9-97d0-d829499abce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4113930"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(raw_documents)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bc438-6003-40c9-ba88-8bc3d3f10e64",
   "metadata": {},
   "source": [
    "### Chroma_db_4: GPT4ALLEmbeddings (22,7M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b364b6e-8cd7-4749-a478-bd78da32858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "emb = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250c500-16d0-4d7d-a85b-ee93b60b9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca706e-4404-4ceb-b87b-7c1f2f0d288f",
   "metadata": {},
   "source": [
    "### Chroma_db_5: bge-large-en (335M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2c42c-4bdf-4d47-a2b6-7dc84a9066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE USA UNA DE LAS GPU\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "#model_kwargs = {'device': 'cuda:1'}\n",
    "model_kwargs = {\"device\": \"cpu\"}  \n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cf69a-e116-4699-9418-48269afe2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f24610-d466-4cb8-871d-d712d264d194",
   "metadata": {},
   "source": [
    "### Chroma_db_3: all-mpnet-base-v2 (109M par√°metros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095d19b-e15e-4d05-8eda-a598e76a19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE USA UNA DE LAS GPU\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "#model_kwargs = {'device': 'cuda:1'}  # specify GPU device\n",
    "model_kwargs = {\"device\": \"cpu\"} \n",
    "emb = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=emb, persist_directory=\"./chroma_db_6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 REAL",
   "language": "python",
   "name": "py310real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
